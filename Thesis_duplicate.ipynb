{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+OyX78lWYtydO9HTojP/q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psingh-xyz/thesis/blob/main/Thesis_duplicate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0RA24q-Vhp4",
        "outputId": "82f070d3-2333-4753-aa08-e46f9241ea8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChangeTokenization'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 13 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (13/13), 9.61 KiB | 9.61 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/psingh-xyz/ChangeTokenization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Python script\n",
        "!python /content/ChangeTokenization/common.py"
      ],
      "metadata": {
        "id": "e7Zb0b37WP0Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG5oUY7Jm__Q",
        "outputId": "a7b4ba54-a0ad-483d-f810-74c8c78665e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyvrBhyBtpk_",
        "outputId": "2e5dcaa0-f4ac-4d33-c808-3edd1089e59d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ChangeTokenization/data_utils.py"
      ],
      "metadata": {
        "id": "3MPMLYvhXHh8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ChangeTokenization/step1.py  --dataset /content/ChangeTokenization/en_wiki.txt --prefix wiki_8k --tok-type unigram --vocab_size 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjQmGmgyvHf-",
        "outputId": "4a0b92d2-0b04-4a5f-cc98-fe3cf3e4091f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ChangeTokenization/en_wiki.txt wiki_8k 8000\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/content/ChangeTokenization/en_wiki.txt --model_prefix=wiki_8k  --vocab_size=8000         --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --model_type=unigram         --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]         --user_defined_symbols=[CLS],[SEP],[MASK]\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /content/ChangeTokenization/en_wiki.txt\n",
            "  input_format: \n",
            "  model_prefix: wiki_8k\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 8000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: [CLS]\n",
            "  user_defined_symbols: [SEP]\n",
            "  user_defined_symbols: [MASK]\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 1\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 0\n",
            "  unk_piece: [UNK]\n",
            "  bos_piece: [BOS]\n",
            "  eos_piece: [EOS]\n",
            "  pad_piece: [PAD]\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: /content/ChangeTokenization/en_wiki.txt\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 38346 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [PAD]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [UNK]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [BOS]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [EOS]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [CLS]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [SEP]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [MASK]\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=10847749\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.95% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=99\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.9995\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 38346 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=5279072\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 150281 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 38346\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 99169\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 99169 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=64834 obj=11.4432 num_tokens=208720 num_tokens/piece=3.2193\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=53028 obj=9.40575 num_tokens=210604 num_tokens/piece=3.97156\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39754 obj=9.36411 num_tokens=220349 num_tokens/piece=5.54281\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=39695 obj=9.33667 num_tokens=220703 num_tokens/piece=5.55997\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=29770 obj=9.39914 num_tokens=238064 num_tokens/piece=7.99678\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=29769 obj=9.37944 num_tokens=238052 num_tokens/piece=7.99664\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=22326 obj=9.46921 num_tokens=256980 num_tokens/piece=11.5103\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=22326 obj=9.44521 num_tokens=256942 num_tokens/piece=11.5086\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16744 obj=9.57728 num_tokens=276892 num_tokens/piece=16.5368\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16744 obj=9.54648 num_tokens=276859 num_tokens/piece=16.5348\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12558 obj=9.71558 num_tokens=296274 num_tokens/piece=23.5925\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12558 obj=9.67727 num_tokens=296221 num_tokens/piece=23.5882\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9418 obj=9.89207 num_tokens=315892 num_tokens/piece=33.5413\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9418 obj=9.84638 num_tokens=315847 num_tokens/piece=33.5365\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=9.89531 num_tokens=320382 num_tokens/piece=36.407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=9.88486 num_tokens=320377 num_tokens/piece=36.4065\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: wiki_8k.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: wiki_8k.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ChangeTokenization/step1.py  --dataset /content/ChangeTokenization/medical/train.dat --prefix medical_8k --tok-type unigram --vocab_size 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X24RdsQovPwi",
        "outputId": "e34770fb-aca6-4412-8821-49cfb6d8b374"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ChangeTokenization/medical/train.dat medical_8k 8000\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/content/ChangeTokenization/medical/train.dat --model_prefix=medical_8k  --vocab_size=8000         --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --model_type=unigram         --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]         --user_defined_symbols=[CLS],[SEP],[MASK]\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /content/ChangeTokenization/medical/train.dat\n",
            "  input_format: \n",
            "  model_prefix: medical_8k\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 8000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: [CLS]\n",
            "  user_defined_symbols: [SEP]\n",
            "  user_defined_symbols: [MASK]\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 1\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 0\n",
            "  unk_piece: [UNK]\n",
            "  bos_piece: [BOS]\n",
            "  eos_piece: [EOS]\n",
            "  pad_piece: [PAD]\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: /content/ChangeTokenization/medical/train.dat\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 14438 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [PAD]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [UNK]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [BOS]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [EOS]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [CLS]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [SEP]\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: [MASK]\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=17797245\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9575% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=71\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999575\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 14438 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=11018606\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 151281 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 14438\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 107064\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 107064 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=58034 obj=11.6816 num_tokens=241393 num_tokens/piece=4.15951\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=47943 obj=9.06433 num_tokens=242132 num_tokens/piece=5.05041\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=35950 obj=9.04832 num_tokens=253782 num_tokens/piece=7.0593\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=35929 obj=9.03311 num_tokens=254625 num_tokens/piece=7.08689\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=26946 obj=9.10939 num_tokens=274989 num_tokens/piece=10.2052\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=26945 obj=9.0923 num_tokens=274932 num_tokens/piece=10.2035\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20207 obj=9.21692 num_tokens=299154 num_tokens/piece=14.8045\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20206 obj=9.19448 num_tokens=299117 num_tokens/piece=14.8034\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15154 obj=9.376 num_tokens=326709 num_tokens/piece=21.5593\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15154 obj=9.34342 num_tokens=326697 num_tokens/piece=21.5585\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11365 obj=9.58529 num_tokens=356166 num_tokens/piece=31.3388\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11365 obj=9.54462 num_tokens=356180 num_tokens/piece=31.3401\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=9.81214 num_tokens=382596 num_tokens/piece=43.4768\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=9.76874 num_tokens=382644 num_tokens/piece=43.4823\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: medical_8k.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: medical_8k.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ChangeTokenization/step2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r38CmVaYclnz",
        "outputId": "869aa46a-0729-47fc-bc5d-cf15fdb07432"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 38460/38460 [00:03<00:00, 11056.64it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/content/change_tokenization/models/rawBert_rawTokenizer/ /content/change_tokenization/models/rawBert_rawTokenizer/Bert_Wiki_8k/\n",
            "  0% 0/603 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "100% 603/603 [19:30<00:00,  1.94s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ChangeTokenization/step3.py"
      ],
      "metadata": {
        "id": "2LbYh93ceQwM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ChangeTokenization/step4.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMzOC6q91Rkh",
        "outputId": "cdb516e7-01f0-4e97-d2f0-d2bf38d822c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/change_tokenization/models/rawBert_rawTokenizer/Bert_Wiki_8k/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/change_tokenization/models/rawBert_rawTokenizer/Bert_Wiki_8k/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ChangeTokenization/step4.py\", line 136, in <module>\n",
            "    main()\n",
            "  File \"/content/ChangeTokenization/step4.py\", line 108, in main\n",
            "    create_experiment(args.mapping_file_1, args.old_vocab_size, args.new_vocab_size, use_one_to_one=True,\n",
            "AttributeError: 'Namespace' object has no attribute 'mapping_file_1'. Did you mean: 'mapping_file_2'?\n"
          ]
        }
      ]
    }
  ]
}